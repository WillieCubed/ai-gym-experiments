{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "An Exploration of Knowledge Graphs and Natural Language Processing in Reinforcement Learning.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP5qdgL1IK79A7MREhGn4yp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WillieCubed/ai-gym-experiments/blob/master/notebooks/knowledge_graphs_and_nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOeQ0xe5h07W",
        "colab_type": "text"
      },
      "source": [
        "# An Exploration of Knowledge Graphs and Natural Language Processing in Reinforcement Learning\n",
        "\n",
        "Based off \"[How to Avoid Being Eaten by a Grue: Structured Exploration Strategies for Textual Worlds](https://arxiv.org/pdf/2006.07409.pdf)\" by Prithviraj Ammanabrolu, Ethan Tien, Matthew Hausknech, and Mark O. Riedl.\n",
        "\n",
        "## Overview\n",
        "Text-adventure games like *Zorkl* pose some challenges:\n",
        "1. **Partial observability**: An agent reasoning about the world solely through incomplete textual descriptions\n",
        "2. **Commonsense reasoning**: An agent using common sense to more intelligently interact with objects in its surroundings\n",
        "3. **A combinatorial state-action space**: Most games have action spaces exeeding a billion possible actions per step\n",
        "\n",
        "Some text-adventure agents like KG-A2C, TDQN, and DRRN use simple exploration strategies like epsilon-greedy or sampling from the distribution of possible actions.\n",
        "\n",
        "The paper focuses on detecting and overcoming bottleneck states. Most text-adventure games have linear plots that involve solving sequences of puzzles to advance a story. To solve puzzles, players explore unlocked areas of the game, collect clues, and acquire tools required to solve them. Puzzles can be viewed as bottlenecks that partition different regions of the state space. Existing RL agents are poorly equipped to solve these types of problems.\n",
        "\n",
        "Quests in text games (and any such sequential descition-making problem requiring long-term dependencies) can be modeled as directed acyclic graphs (DAGs) in which verticies indicate rewards that can be collected or dependencies that must be satisfied to progress. Text-adventure games have two types of dependencies\n",
        "\n",
        "## Related Work\n",
        "\n",
        "## Q*BERT\n",
        "Q*BERT is a reinforcement learning algorithm based on KG-A2C. It uses a knowledge graph to represent its understanding of the world state.\n",
        "\n",
        "A knowledge graph is a set of relations \n",
        "$\\langle s, r, o \\rangle$ such that $s$ is a subject, $r$ is a relationship, and $o$ is some object.\n",
        "\n",
        "Q*BERT uses a variant of the BERT language transformer to answer questions about the current state and populate the knowledge graph from the answers.\n",
        "\n",
        "### Knowlege Graph State Representation\n",
        "\n",
        "\n",
        "- The language model ALBERT was used.\n",
        "  - Fine-tuned for QA on the SQuAD dataset.\n",
        "  - Fine-tuned again on Jericho-QA, a dataset created by making question answering pairs about text-games\n",
        "  - Jericho is a framework for RL in text-games.\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMee0HIjDHJE",
        "colab_type": "text"
      },
      "source": [
        "# TODO List\n",
        "- Set up Jericho\n",
        "- Set up Q-BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdG-R-088Rg_",
        "colab_type": "text"
      },
      "source": [
        "## KG-A2C\n",
        "### Overview\n",
        "\n",
        "From \"[Graph Constrained Reinforcement Learning for Natural Language Action Spaces](https://openreview.net/pdf?id=B1x6w0EtwH)\"\n",
        "KG-A2C is an agent that builds a knowledge graph while exploring and generates actions using a template-based action space.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryo45Ao4Hd8l",
        "colab_type": "text"
      },
      "source": [
        "## KG-A2C Architecuture\n",
        "KG-A2C consists of three units:\n",
        "- Input representation containing:\n",
        "  - Observation encoder\n",
        "  - Score encoder\n",
        "  - Knowledge graph\n",
        "- Action decoder\n",
        "- Critic\n",
        "\n",
        "### Input Representation\n",
        "At every step, an observation of the room description, game feedback, inventory, and previous action $o_t = ({o_t}_{desc}, {o_t}_{game}, {o_t}_{inv}, a_{t-1})$ is received along with a total score $R_t$.\n",
        "\n",
        "- ${o_t}_{desc}$ is a textual description of the agent's location, corresponds to command \"look\"\n",
        "- ${o_t}_{game}$ is the simulator's response to the agent's previous action and consists of narrative text\n",
        "- ${o_t}_{inv}$ and ${a_{t-1}}$ inform the agent about the contents of its inventory and previous action, respectively\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BopLhwp2ROVJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Setting up Depdendencies\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glpdgin-RYuX",
        "colab_type": "text"
      },
      "source": [
        "## Training\n",
        "Every step, a knowledge graph $\\mathit{G_{t}}$\n",
        "\n",
        "$\\mathit{H}=\\{\\mathbf{h_1}, \\mathbf{h_2}, ... \\mathbf{h_N} | \\mathbf{h}_i\\}$ where $\\mathit{N}$ is the number of nodes and $\\mathit{F}$ is the number of features in each node consisting of the average subword embeddings of the entity.(?)\n",
        "\n",
        "\n",
        "$\\mathit{e_{ij}} = \\mathit{LeakyReLU}(\\mathbf{p} \\mathit{W}(\\mathbf{h_i} \\oplus \\mathbf{h_j})$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JSLvuosLzQR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def edge(start, end, features):\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DJ9fIQUgqle",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Environment setup {display-mode: \"form\"}\n",
        "\n",
        "import spacy.cli\n",
        "import torch.nn as nn\n",
        "\n",
        "# Q-BERT setup\n",
        "spacy.cli.download('en_core_web_sm')\n",
        "\n",
        "!wget https://github.com/BYU-PCCL/z-machine-games/archive/master.zip\n",
        "!unzip master.zip\n",
        "\n",
        "!pip install jericho\n",
        "!java"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4sTTE0hLl63",
        "colab_type": "text"
      },
      "source": [
        "# Implementing KG-A2C\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8QB58_BHO0i",
        "colab_type": "text"
      },
      "source": [
        "# Implementing the Grue Paper\n",
        "Now we begin using what we know about KG-2AC to create an agent that can play Zork I."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Kr1Z26HsoyH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Training parameters\n",
        "#@markdown Things that affect model training.\n",
        "\n",
        "MAX_TIMESTEPS = 10000 #@param {type: \"number\", min: 1}\n",
        "MAX_PATIENCE = 100 #@param {type: \"number\", min: 1}\n",
        "#@markdown ---"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z64tbLBF1dOW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Main training loop\n",
        "\n",
        "def train():\n",
        "    pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zoshgri1hQ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a2c = ...\n",
        "\n",
        "def qbert_update(state_t, policy):\n",
        "    \"\"\"Update after each step.\n",
        "    \n",
        "    Params:\n",
        "        state_t: The state at a given time step\n",
        "        policy:\n",
        "    \"\"\"\n",
        "    next_state, reward_g_t = env.step(state_t, policy)\n",
        "    reward = calculate_reward(next_state, reward_g_t)\n",
        "    a2c.update(policy, reward)\n",
        "    return next_state, reward, policy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYt2GAk_1kPN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def backtrack(backtrack_policy, backtrack_buffer):\n",
        "    \"\"\"Attempt to overcome a bottleneck.\n",
        "    Params:\n",
        "        backtrack_policy:\n",
        "        backtrack_buffer: \n",
        "    \"\"\"\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0bSz5141hIT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = ...\n",
        "\n",
        "def structured_exploration():\n",
        "    chained_policy, backtrack_policy, current_policy = ...\n",
        "    backtrack_state_buffer = []\n",
        "    current_state_buffer = []\n",
        "\n",
        "    state, reward = env.reset()\n",
        "\n",
        "    def return(policy):\n",
        "        return 0\n",
        "\n",
        "    max_return = reward\n",
        "    patience = 0\n",
        "    for t in range(MAX_TIMESTEPS):\n",
        "        next_state, reward, current_policy = qbert_update(state, current_policy)\n",
        "        current_state_buffer.append(next_state)\n",
        "        patience += 1\n",
        "        if max_return(policy) <= max_reward:\n",
        "            if patience >= MAX_PATIENCE:\n",
        "                state, max_reward, current_policy = backtrack(backtrack_policy, backtrack_buffer)\n",
        "                chained_policy += current_policy\n",
        "        current_return = return(current_policy)\n",
        "        if current_return > max_return:  # New high score found\n",
        "            max_return = current_reward\n",
        "            backtrack_policy = current_policy\n",
        "            backtrack_state_buffer = current_state_buffer\n",
        "            patience = 0\n",
        "    return chained_policy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hp6jZMRNxUPt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class QBERT:\n",
        "    def choose_action(state):\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}